{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXEepBs92lWjZrWstBYhAY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham519/jupyter-notebook_python/blob/main/Tokennization_and_lemmetization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy  nltk  spacy "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6y5SbGc1R1U",
        "outputId": "081273ea-f48a-47f4-e2e2-72334259a583"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "igPrl-inz2fd"
      },
      "outputs": [],
      "source": [
        "#This will be the orignal corpus \n",
        "\n",
        "corpus_original = \"Need to finalize the demo corpus which be used for this notebook and it should give us some of the words at the end !! . It should be done by this month . but wiil it? this notebook has been run 4 times.\"\n",
        "corpus = \"Need to finalize the demo corpus which be used for this notebook & it should give us some of the words at the end!! . It should be done by this month . but wiil it? this notebook has been run 4 times.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lower case the corpus \n",
        "corpus = corpus.lower()\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6PEosyt0W_z",
        "outputId": "76253356-f7c4-4aec-cd16-090542cb18e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the demo corpus which be used for this notebook & it should give us some of the words at the end!! . it should be done by this month . but wiil it? this notebook has been run 4 times.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing digits in the corpus \n",
        "import re \n",
        "corpus = re.sub(r'\\d+' , '' , corpus)\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07FCGQSv1qSV",
        "outputId": "1dec6307-d13c-4dc0-aa6c-35e0f277ed55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the demo corpus which be used for this notebook & it should give us some of the words at the end!! . it should be done by this month . but wiil it? this notebook has been run  times.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing digits in the corpus\n",
        "import string \n",
        "corpus = corpus.translate(str.maketrans('' , '' , string.punctuation))\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DQmGJYr0cyW",
        "outputId": "8c119e4b-98c4-4776-f1b1-2b85df984e64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the demo corpus which be used for this notebook  it should give us some of the words at the end  it should be done by this month  but wiil it this notebook has been run  times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing trailing whitespaces\n",
        "corpus = ' '.join([token for token in corpus.split()])\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MK3GtVkx0-23",
        "outputId": "0642c3b3-def0-423e-8add-c608a2b8ca27"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'need to finalize the demo corpus which be used for this notebook it should give us some of the words at the end it should be done by this month but wiil it this notebook has been run times'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqzgvBWX1KDq",
        "outputId": "4954a035-30ce-4e64-e1b3-7ec69cd4a6ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-28 04:11:12.228364: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-28 04:11:14.835741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "\n",
        "tokenized_corpus_nltk = word_tokenize(corpus)\n",
        "print(\"\\nNLTK\\nTokenized corpus:\" , tokenized_corpus_nltk)\n",
        "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
        "print(\"Tokenized corpus without stopwords: \" , tokenized_corpus_without_stopwords)\n",
        "\n",
        "##Spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import spacy \n",
        "spacy_model = spacy.load('en_core_web_sm')\n",
        "\n",
        "stopwords_spacy = spacy_model.Defaults.stop_words\n",
        "print(\"\\nSpacy:\")\n",
        "tokenized_corpus_spacy = word_tokenize(corpus)\n",
        "print(\"Tokenized Corpus: \" , tokenized_corpus_spacy)\n",
        "tokens_without_sw = [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
        "\n",
        "print(\"Tokenized corpus without stopwords\" , tokens_without_sw)\n",
        "\n",
        "print(\"Difference between NLTK and spacy output: \\n\" ,\n",
        "      set(tokenized_corpus_without_stopwords) - set(tokens_without_sw))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wx-NfZPj2TMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12116dd-4728-497c-bc29-2e7ab3e3eb2c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK\n",
            "Tokenized corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'be', 'used', 'for', 'this', 'notebook', 'it', 'should', 'give', 'us', 'some', 'of', 'the', 'words', 'at', 'the', 'end', 'it', 'should', 'be', 'done', 'by', 'this', 'month', 'but', 'wiil', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
            "Tokenized corpus without stopwords:  ['need', 'finalize', 'demo', 'corpus', 'used', 'notebook', 'give', 'us', 'words', 'end', 'done', 'month', 'wiil', 'notebook', 'run', 'times']\n",
            "\n",
            "Spacy:\n",
            "Tokenized Corpus:  ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'be', 'used', 'for', 'this', 'notebook', 'it', 'should', 'give', 'us', 'some', 'of', 'the', 'words', 'at', 'the', 'end', 'it', 'should', 'be', 'done', 'by', 'this', 'month', 'but', 'wiil', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
            "Tokenized corpus without stopwords ['need', 'finalize', 'demo', 'corpus', 'notebook', 'words', 'end', 'month', 'wiil', 'notebook', 'run', 'times']\n",
            "Difference between NLTK and spacy output: \n",
            " {'done', 'us', 'give', 'used'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now using the Stemming "
      ],
      "metadata": {
        "id": "gtlmBXERGhQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(\"Before Stemming: \")\n",
        "print(corpus)\n",
        "\n",
        "print(\"After Stemming: \")\n",
        "for word in tokenized_corpus_nltk:\n",
        "  print(stemmer.stem(word), end=\" \")\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXLF4HNYGMGi",
        "outputId": "8bc8d6e8-332a-4799-e859-0e561261af70"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Stemming: \n",
            "need to finalize the demo corpus which be used for this notebook it should give us some of the words at the end it should be done by this month but wiil it this notebook has been run times\n",
            "After Stemming: \n",
            "need to final the demo corpu which be use for thi notebook it should give us some of the word at the end it should be done by thi month but wiil it thi notebook ha been run time "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "brH3Qh4VG9x8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for word in tokenized_corpus_nltk:\n",
        "  print(lemmatizer.lemmatize(word) , end=\" \")\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HyfbPuBG7pT",
        "outputId": "852967b8-61ff-4764-fa18-bc05b1ec7057"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the demo corpus which be used for this notebook it should give u some of the word at the end it should be done by this month but wiil it this notebook ha been run time "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS Tagging \n"
      ],
      "metadata": {
        "id": "vln9WpfVHex3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pos tagging using spacy \n",
        "print(\"POS Tagging using spacy: \")\n",
        "doc = spacy_model(corpus_original)\n",
        "\n",
        "#Token and Tag\n",
        "for token in doc :\n",
        "  print(token, \":\" , token.pos_)\n",
        "\n",
        "#Pos tagging using nltk \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(\"POS Tagging using NLTK: \")\n",
        "pprint(nltk.pos_tag(word_tokenize(corpus_original)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7IvkwbcHapR",
        "outputId": "141a6039-f6bf-4702-d7c5-e11088421ca3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging using spacy: \n",
            "Need : VERB\n",
            "to : PART\n",
            "finalize : VERB\n",
            "the : DET\n",
            "demo : NOUN\n",
            "corpus : NOUN\n",
            "which : PRON\n",
            "be : AUX\n",
            "used : VERB\n",
            "for : ADP\n",
            "this : DET\n",
            "notebook : NOUN\n",
            "and : CCONJ\n",
            "it : PRON\n",
            "should : AUX\n",
            "give : VERB\n",
            "us : PRON\n",
            "some : PRON\n",
            "of : ADP\n",
            "the : DET\n",
            "words : NOUN\n",
            "at : ADP\n",
            "the : DET\n",
            "end : NOUN\n",
            "! : PUNCT\n",
            "! : PUNCT\n",
            ". : PUNCT\n",
            "It : PRON\n",
            "should : AUX\n",
            "be : AUX\n",
            "done : VERB\n",
            "by : ADP\n",
            "this : DET\n",
            "month : NOUN\n",
            ". : PUNCT\n",
            "but : CCONJ\n",
            "wiil : VERB\n",
            "it : PRON\n",
            "? : PUNCT\n",
            "this : DET\n",
            "notebook : NOUN\n",
            "has : AUX\n",
            "been : AUX\n",
            "run : VERB\n",
            "4 : NUM\n",
            "times : NOUN\n",
            ". : PUNCT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging using NLTK: \n",
            "[('Need', 'NN'),\n",
            " ('to', 'TO'),\n",
            " ('finalize', 'VB'),\n",
            " ('the', 'DT'),\n",
            " ('demo', 'NN'),\n",
            " ('corpus', 'NN'),\n",
            " ('which', 'WDT'),\n",
            " ('be', 'VB'),\n",
            " ('used', 'VBN'),\n",
            " ('for', 'IN'),\n",
            " ('this', 'DT'),\n",
            " ('notebook', 'NN'),\n",
            " ('and', 'CC'),\n",
            " ('it', 'PRP'),\n",
            " ('should', 'MD'),\n",
            " ('give', 'VB'),\n",
            " ('us', 'PRP'),\n",
            " ('some', 'DT'),\n",
            " ('of', 'IN'),\n",
            " ('the', 'DT'),\n",
            " ('words', 'NNS'),\n",
            " ('at', 'IN'),\n",
            " ('the', 'DT'),\n",
            " ('end', 'NN'),\n",
            " ('!', '.'),\n",
            " ('!', '.'),\n",
            " ('.', '.'),\n",
            " ('It', 'PRP'),\n",
            " ('should', 'MD'),\n",
            " ('be', 'VB'),\n",
            " ('done', 'VBN'),\n",
            " ('by', 'IN'),\n",
            " ('this', 'DT'),\n",
            " ('month', 'NN'),\n",
            " ('.', '.'),\n",
            " ('but', 'CC'),\n",
            " ('wiil', 'VBD'),\n",
            " ('it', 'PRP'),\n",
            " ('?', '.'),\n",
            " ('this', 'DT'),\n",
            " ('notebook', 'NN'),\n",
            " ('has', 'VBZ'),\n",
            " ('been', 'VBN'),\n",
            " ('run', 'VBN'),\n",
            " ('4', 'CD'),\n",
            " ('times', 'NNS'),\n",
            " ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cca1XNZ3IGlZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}