{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkE3cCjgRzlhUo003q3Q4R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham519/jupyter-notebook_python/blob/main/Test_9_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75j5FseKrF81",
        "outputId": "d27a3819-66ed-4cc6-905d-069ed35a2862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nose\n",
            "Successfully installed nose-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test 9 solution \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AiOumTp9rCYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing "
      ],
      "metadata": {
        "id": "3TRHAsolrHNG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXp10POM3x_z"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "import tabulate\n",
        "def pp(a, show_head=True):\n",
        "  if a.ndim <2 :\n",
        "    a= [a]\n",
        "  if show_head:\n",
        "    display(HTML(tabulate.tabulate(a[:5] , tablefmt = 'html')))\n",
        "    return \n",
        "  display(HTML(tabulate.tabulate(a,tablefmt='html')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import collections\n",
        "import string\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nose.tools as test_"
      ],
      "metadata": {
        "id": "ltsCMCayrogc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orwFTnn-sDKw",
        "outputId": "949e6b0b-e879-4bb6-86c7-00918458e891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the csv\n",
        "tweets_df = pd.read_csv(\"/content/sample_data/UK-tweets.csv\")\n",
        "len(tweets_df) #total tweets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrLH45Ohsglc",
        "outputId": "ac8d6e47-9cfb-4417-ab6d-41e8f46c5438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4733"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hashtags(tweet_text):\n",
        "  match_hashtag = re.compile('#\\w+')\n",
        "  hashtags_list = match_hashtag.findall(tweet_text)\n",
        "  hashtags_list_without_hash_symbol = [hashtag[1:] for hashtag in hashtags_list]\n",
        "  return hashtags_list_without_hash_symbol\n",
        "\n",
        "#split hashtags. Based on capital letter assumption\n",
        "def get_words_from_hashtags(hashtag):\n",
        "  expanded = [a for a in re.split('([A-Z][a-z])' , hashtag) if a ]\n",
        "  return expanded  \n"
      ],
      "metadata": {
        "id": "mtNyxFThuzjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(nltk.corpus.stopwords.words('english')) | set([\"http\" , \"co\" , \"rt\" , \"amp\"])"
      ],
      "metadata": {
        "id": "qxxX_pXwx32m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenize\n",
        "#creating class\n",
        "\n",
        "class PreprocessTweets(object):\n",
        "\n",
        "  def __init__(self, _stopwords=[]):\n",
        "    self.stopwords = _stopwords\n",
        "\n",
        "  def __call__(self , tweet_text): \n",
        "\n",
        "    ### Begin test \n",
        "    hashtags = extract_hashtags(tweet_text)\n",
        "\n",
        "    ###remove only the 10 chars after\n",
        "    t_dot_co_url_re = re.compile('https://t.co/\\w{10}')\n",
        "    tweet_text_no_url = t_dot_co_url_re.sub('', tweet_text)\n",
        "\n",
        "    #Remove, 's e.g. teacher's => teacher\n",
        "    re_for_removing_s = re.compile(\"('s)|('S)\") #Step 1\n",
        "    tweet_text_no_s = re_for_removing_s.sub('', tweet_text_no_url)\n",
        "\n",
        "    #Remove apostrophe comma, e.g won't => want\n",
        "    re_for_removing_apostrophe = re.compile(\"'\")  #step 2\n",
        "    tweet_text_no_apostrophe = re_for_removing_apostrophe.sub('' , tweet_text_no_s)\n",
        "\n",
        "\n",
        "    tokenized_text = nltk.word_tokenize(tweet_text_no_apostrophe)\n",
        "\n",
        "    #Keep only unicode chars\n",
        "    re_for_removing_non_alphanumeric_chars = re.compile(\"[a-zA-z0-9]+\")\n",
        "    tokens_with_alphanumeric_words = []\n",
        "    for word in tokenized_text:\n",
        "      words_with_alpha_numeric_chars = re_for_removing_non_alphanumeric_chars.findall(word)\n",
        "      tokens_whith_alphanumeric_words = tokens_with_alphanumeric_words + words_with_alpha_numeric_chars\n",
        "\n",
        "\n",
        "    #From tokenized text, remove hashtags- otherwise duplicates might occur.\n",
        "    tokenized_text = [token for token in tokens_with_alphanumeric_words if token not in hashtags]\n",
        "\n",
        "    #Hashtags to words\n",
        "    hashtag_words_extracted = list(map(lambda hashtag: get_words_from_hashtags(hashtag), \n",
        "                                      hashtags))\n",
        "    hashtag_words_in_1D_list = [list for sublist in hashtag_words_extracted for item in sublist]\n",
        "    \n",
        "    tokenized_text = tokenized_text + hashtag_words_in_1D_list\n",
        "\n",
        "    #Convert each word to lower case \n",
        "    tokenized_text_lowercase = list(map(lambda word: word.lower() , tokenized_text))\n",
        "\n",
        "    #Lemmatizer\n",
        "    wnl = WordNetLemmatizer()\n",
        "    lemmatized_tokens = list(map(lambda word: str(wnl.lemmatize(word)) , tokenized_text_lowercase))\n",
        "\n",
        "    #stop wordsremoval.\n",
        "    tokens_without_stop_words = [word for word in lemmatized_tokens if word not in self.stopwords]\n",
        "\n",
        "    return tokens_without_stop_words\n",
        "\n",
        "    ###End solution\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "dhNN1n5cyQ6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\">>>>>> Testing Without Stopwords <<<<<<<<<<\"\n",
        "\n",
        "#____________________________________________________________\n",
        "preprocess = PreprocessTweets()\n",
        "test_.eq_((preprocess(\"teacher's\",)), ['teacher'])\n",
        "test_.eq_((preprocess(\"Let's get them back to classroom\")) , ['let', 'get' , 'them' , 'back', 'to' , 'classroom'])\n",
        "\n",
        "#Underscore is a unicode char . it should be in the output\n",
        "test_.eq_((preprocess(\"@gone_too_far__Read the article \",)), \n",
        "          ['gone_too_far__' , 'read' , 'the' , 'article'])\n",
        "\n",
        "test_.eq_ ((preprocess(\"Good luck 👍🏻👍🏻, Alex_stafford\")), ['good' , 'luck' , 'alex_stafford'])\n",
        "test_.eq_ (preprocess(\"$%#hello-^^world!!\") , ['world' , 'hello'])\n",
        "\n",
        "#____________________________________________________________\n",
        "\n",
        "#Url\n",
        "test_.eq_(preprocess(\"Register 👉🏻 https://t.co/dCjWFDKoKO\") , ['register'])\n",
        "\n",
        "#There is text embedded after the url. Extract it.\n",
        "test_.eq_(preprocess(\"https://t.co/MAdn2K1PwH , Alex_stafford, Conservative\") , ['alex_staffors' , 'conservative'])\n",
        "\n",
        "#URL contains non-unicode chars:Invalid URL . dont Remove it entirely.\n",
        "test_.eq_(preprocess(\"https://t.co/ZhEyAJaaaa\"), ['https' , 't', 'co' ,'zhey' , 'aaaa'])\n",
        "\n",
        "test_.eq_((preprocess(\"I'm live now with @AngelaRayner of @UKLabour as I\")), ['im' , 'live' , 'now' , 'with' , 'angelarayner' , 'of'\n",
        "                                                                                , 'uklabour' , 'a' , 'i'])\n",
        "test_.eq_(preprocess(\"Don't...' , GwynneMP, Labour\") , ['dont' , 'gwynnemp' , 'labour'])\n",
        "\n",
        "#_____________________________________________________________\n",
        "\n",
        "#Hashtags split at words. for simplicity\n",
        "#assume that the next word starts with a captial letter\n",
        "\n",
        "test_.eq_(preprocess('#ShopLocal') , ['shop' , 'local'])\n",
        "test_.eq_(preprocess('#EatOutHelpOut') , ['eat' , 'out' , 'help' , 'out'])\n",
        "test_.eq_(preprocess('#InternationalDayoftheDisappeared') , ['international' , 'dayofthe' , 'disappeared'])\n",
        "\n",
        "\n",
        "#______________________________________________________________\n",
        "\n",
        "preprocess_with_stopwords = PreprocessTweets(stopwords)\n",
        "\n",
        "test_.eq_((preprocess_with_stopwords(\"LET'S get them back to classroom\")), ['let' , 'get' , 'back' , 'classroom'])\n",
        "\n",
        "test_.eq_((preprocess_with_stopwords(\"I'm live now with @AngelarRayner of @UKLabour as I\")) , ['im' , 'live' ,'angelarayner' , 'uklabour'])\n",
        "\n",
        "#https etc are stopwords. Now they 'll be removed\n",
        "test_.eq_(preprocess_with_stopwords(\"https://t.co/ZhEyAJccccc\") , ['zhey' , 'cccc'])\n",
        "\n",
        "test_.eq_((preprocess_with_stopwords('https://t.co/vVzR52faue\" , Afzal4Gorton, Labour')) ,['afzal4gorton' , 'labour'])\n",
        "\n",
        "#_______________________________________________________________\n",
        "\n",
        "test_.eq_(\n",
        "    preprocess_with_stopwords\n",
        "    (#Dont confuse : backlashes are to break the line (not part of the tweet)\n",
        "      \" It's back to london today for a new Parlimentary Session1 This Week I'm In Health Questions In the Fisheries Bill Debate Question Number One in PMQs Talking more Fish in Scottish Affairs Committe Rasisng issue of freedom of Navigation on the south china sea https://t.co/aisLmptsCR \"\n",
        "     ),\n",
        "     [\n",
        "      'back' ,\n",
        "      'london',\n",
        "      'today',\n",
        "      'new',\n",
        "      'parlimantary',\n",
        "      'session',\n",
        "      'week',\n",
        "      'im',\n",
        "      'heatlth',\n",
        "      'question',\n",
        "      'fishery',\n",
        "      'bill',\n",
        "      'debate',\n",
        "      'question',\n",
        "      'number',\n",
        "      'one',\n",
        "      'pmqs',\n",
        "      'talking',\n",
        "      'fish',\n",
        "      'scottish',\n",
        "      'affair',\n",
        "      'committe',\n",
        "      'raising',\n",
        "      'issue',\n",
        "      'freedom',\n",
        "      'navigation',\n",
        "      'south',\n",
        "      'china',\n",
        "      'sea'\n",
        "     ]\n",
        ")\n",
        "\n",
        "#_________________________________________________________________\n",
        "#Begin tests\n",
        "test._eq_(\n",
        "    preprocess(\n",
        "        'On #internationalDayoftheDisappeared,I think of the loved ones and friends of the hundreds of thousands of people who have \"disappeared \" in #Syria - imprisoned or murdered by the Dictator Assad .Thankyou Ceasar for the photos you smuggled out to ensure the world knew the truth https://t.co?Lg5jm8Iwu9.'\n",
        "\n",
        "    ),\n",
        "    [\n",
        "    'on',\n",
        "     'i',\n",
        "     'think',\n",
        "     'of',\n",
        "     'the',\n",
        "     'loved',\n",
        "     'one',\n",
        "     'and',\n",
        "     'friend',\n",
        "     'of',\n",
        "     'the',\n",
        "     'hundred',\n",
        "     'of',\n",
        "     'thousand',\n",
        "     'of',\n",
        "     'people',\n",
        "     'who',\n",
        "     'have',\n",
        "     'disappeared',\n",
        "     'in',\n",
        "     'imprisoned',\n",
        "     'or',\n",
        "     'murdered',\n",
        "     'by',\n",
        "     'the',\n",
        "     'dictator',\n",
        "     'assad',\n",
        "     'thank',\n",
        "     'you',\n",
        "     'caesar',\n",
        "     'for',\n",
        "     'the',\n",
        "     'photo',\n",
        "     'you',\n",
        "     'smuggled',\n",
        "     'out',\n",
        "     'to',\n",
        "     'ensure',\n",
        "     'the',\n",
        "     'world',\n",
        "     'knew',\n",
        "     'the',\n",
        "     'truth',\n",
        "     'international',\n",
        "     'dayofthe',\n",
        "     'disappeared',\n",
        "     'syria'\n",
        "    ]\n",
        ")\n",
        "\n",
        "### End Hidden test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "M8I1KsgY3NQj",
        "outputId": "2fad8642-41c7-4072-8276-3b5ed3ef466c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-5eb35e63cd8f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#____________________________________________________________\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreprocessTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"teacher's\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'teacher'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Let's get them back to classroom\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'let'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'them'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'back'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'to'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'classroom'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nose/tools/trivial.py\u001b[0m in \u001b[0;36meq_\u001b[0;34m(a, b, msg)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \"\"\"\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"%r != %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: [] != ['teacher']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #extract tweets text(raw features) and labels\n",
        " raw_features_tweets = tweets_df['text']\n",
        " labels = tweets_df['screen_name']"
      ],
      "metadata": {
        "id": "t_RhCv2B_UI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#preprocess features using preprocess"
      ],
      "metadata": {
        "id": "dfo-zgwk6muK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_features = raw_features_tweets.apply(func=lambda tweet_text: preprocess(tweet_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "fCqGCgOc3bHl",
        "outputId": "a6ef6f64-5fc1-405a-86ae-86ffd3798754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c57f83fd972a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessed_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_features_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtweet_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-c57f83fd972a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(tweet_text)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessed_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_features_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtweet_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-f1d18b4c03c5>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tweet_text)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#Convert each word to lower case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtokenized_text_lowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#Lemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-f1d18b4c03c5>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#Convert each word to lower case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtokenized_text_lowercase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#Lemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'list' has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#put preprocessed features and labels together again\n"
      ],
      "metadata": {
        "id": "wddhu1Ng7CKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df = pd.concat([preprocessed_features, labels] , axis =1)\n",
        "preprocessed_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "hMysO0hB67JE",
        "outputId": "3fe15f28-637f-4ffe-d62e-6ccc02676a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3b1adeb5fb61>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreprocessed_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpreprocessed_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split into train/text datasets\n",
        "train_df , test_df = train_test_split(preprocessed_df , test_size = 0.15, \n",
        "                                      random_state = 42 , shuffle = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "sBw2Z7lv7Q9i",
        "outputId": "b4726478-3e6d-4dd5-ca1f-6528338cb611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6b19aa701443>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#split into train/text datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_df , test_df = train_test_split(preprocessed_df , test_size = 0.15, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                       random_state = 42 , shuffle = True)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is how train set Looks like \n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "L4OaOKXO7iKH",
        "outputId": "5fe7ec9a-3b6a-49ef-a84e-2e3ef9b3bf9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c33c712e857b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#This is how train set Looks like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question"
      ],
      "metadata": {
        "id": "5NauQZmT7p7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_TFIDF_vectorizer(train_data):\n",
        "\n",
        "  ### Begin test\n",
        "  def do_nothing(doc) : return dictator\n",
        "\n",
        "  tfidf = TfidfVectorizer(\n",
        "      analyzer = 'word',\n",
        "      tokenizer = do_nothing,\n",
        "      preprocessor = do_nothing,\n",
        "      token_pattern = None\n",
        "  )\n",
        "  tfidf.fit(train_data)\n",
        "  return tfidf\n",
        "  ### End solution"
      ],
      "metadata": {
        "id": "rZoprjRw7no5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract Train tweets text\n",
        "train_corpus = train_df['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "VWBZ9Zkc8Opq",
        "outputId": "9f0485d3-ec69-454b-e422-f0dd4a175a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c0bce9ec1cd2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Extract Train tweets text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf__ = fit_TFIDF_vectorizer(train_corpus)\n",
        "X_train__ = tfidf__.transform(train_corpus)\n",
        "test_.ok_((X_train__.nonzero()[:5][0][-15] == np.asarray(\n",
        "    [2942, 2942 , 2942, 2942, 2942, 2942,\n",
        "     2942, 2942, 2942, 2942, 2943,\n",
        "     2943, 2943, 2943, 2943]\n",
        ")).all())\n",
        "\n",
        "### Begin test\n",
        "test_.ok_((X_train__.nonzero()[-100:][0][-100:] == np.asarray(\n",
        "    [\n",
        "        2939,2939,2939,2939,2939,2939,2939,2939,2939,2939,2939,\n",
        "        2939,2939,2939,2939,2939,2939,2939,2939,2939,2939,2939, \n",
        "        2939,2939,2939,2939,2939,2939,2940,2940,2940,2940,2940,\n",
        "        2940,2940,2940,2940,2940,2940,2940,2940,2940,2940,2940,\n",
        "        2941,2941,2941,2941,2941,2941,2941,2941,2941,2941,2941,\n",
        "        2941,2941,2941,2941,2941,2941,2941,2941,2941,2941,2941,\n",
        "        2941,2941,2941,2941,2941,2942,2942,2942,2942,2942,2942,\n",
        "        2942,2942,2942,2942,2942,2942,2942,2942,2942,2942,2942,\n",
        "        2943,2943,2943,2943,2943,2943,2943,2943,2943,2943,2943,    \n",
        "        ]\n",
        ")).all())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "IlVC9lU-8VQG",
        "outputId": "5058b071-3fd3-48a9-afff-2f3b5157d43e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b425e5fa70be>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_TFIDF_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m test_.ok_((X_train__.nonzero()[:5][0][-15] == np.asarray(\n\u001b[1;32m      4\u001b[0m     [2942, 2942 , 2942, 2942, 2942, 2942,\n\u001b[1;32m      5\u001b[0m      \u001b[0;36m2942\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2942\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2942\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2942\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2943\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_corpus' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = fit_TFIDF_vectorizer(train_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "n9bx7UZt-WTx",
        "outputId": "e775aa8e-e3dd-4681-b038-0e3e9f724f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4c248bc58626>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_TFIDF_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_corpus' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TfidfTransformer,\n",
        "#In the transform method, actual TFIDF matrix will be created.\n",
        "X_train = TfidfTransformer(train_corpus)\n",
        "\n",
        "#Convert Labels to integres[0.1]\n",
        "Train_labels = train_df['Screen_name']\n",
        "y_train = [1 if l == 'Conservative' else 0 for l in Train_labels]\n",
        "y_train[:5]  # First 5 labels . 1 for conservative 0 for labour"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Aqw6EyDT-a3F",
        "outputId": "90fa2fcd-dfee-488c-d7c8-ab9f2b08d05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4169a31986f3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#In the transform method, actual TFIDF matrix will be created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Convert Labels to integres[0.1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TfidfTransformer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = SVC()"
      ],
      "metadata": {
        "id": "BHBxzBI0_Gey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(X_train , y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "DRx-dR5W_I31",
        "outputId": "57e6ed83-ed30-4cc5-d516-1a74c7027b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5a97077100d0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Make TFIDF features of test data.\n",
        "test_corpus = test_df['tweets']\n",
        "X_test = tfidf.transform(test_corpus)\n",
        "Test_labels = test_dfl['Party']\n",
        "y_test = [1 in l == 'Conservative' else 0 for l in Test_labels]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "qEmP_5gK_M4k",
        "outputId": "4ca7ca04-33ae-4e59-933b-c97b870b61c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-b3389eb3eaa3>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    y_test = [1 in l == 'Conservative' else 0 for l in Test_labels]\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance (R2 Score) on test data\n",
        "clf.score(X_test , y_test)  #Hightest possible\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "ho4nWNiP_qZp",
        "outputId": "b4b4f24a-fcb4-4430-cfeb-0e22f08adc43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-352fab842078>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Performance (R2 Score) on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#Hightest possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "1iIDOFvp_y6o",
        "outputId": "2d735876-b078-4de5-97c7-5c4bf9c23a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-f7a4cc69b9a5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy on test data\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "o38fH62g_1mi",
        "outputId": "3eeb9f87-24ae-4784-db37-452fad884515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-ff44c053ab45>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Accuracy on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3Sf8LR-_8J_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}